{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "H0kj-8xxnORC",
        "GF8Ens_Soomf",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "r2jJGEOYphqO",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swastik02aug/unsupervised-netflix-project/blob/main/ML_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows: Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -Swastik Chaurasia**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service's number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Approaches:-\n",
        "\n",
        "Step 1 - Viewing and cleaning data being the initials, we started with importing necessary libraries, mounting drive and storing data in variables for deriving meaningful insights. Next step being data analysis and visualization, where we analyzed our data distribution as univariate, bivariate and multivariate plots. Multicollinearity check was performed.\n",
        "\n",
        "Step 2 - We performed 3 hypothesis testing one comparing means for two variavles using T-test , second was asssociation of target age and country using chi_square test, last was asssociation of target age and duration using chi_square test.\n",
        "\n",
        "Step 3 - Presence of null values would have created possible errors in the further steps, so we replaced few values with null, few with the word \"unknown\" few were dropped.\n",
        "\n",
        "Step 4 - As a part of feature engineering we have done textual data processing which includes:expand contraction, lower casing, removing punctuations and stopwords, normalization,and vectoriztion. We performed data scaling using standardscalar, and dimensionality reduction using PCA.\n",
        "\n",
        "Step 5 - Last step was to perform clustering using different algorithms, we tried K-Means, ElbowCurve, DBSCAN, Dendogram, Agglomerative Clustering. Recommendation system function was also developed."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search.\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.\n",
        "\n",
        "In this project, we are required to do Exploratory Data Analysis\n",
        "\n",
        "Understanding what type content is available in different countries\n",
        "\n",
        "Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "\n",
        "Clustering similar content by matching text-based features.\n",
        "\n",
        "Attribute Information Show_id : Unique ID for every Movie / Tv Show\n",
        "\n",
        "Type : Identifier - A Movie or TV Show\n",
        "\n",
        "Title : Title of the Movie / Tv Show\n",
        "\n",
        "Director : Director of the Movie\n",
        "\n",
        "Cast : Actors involved in the movie / show\n",
        "\n",
        "Country : Country where the movie / show was produced\n",
        "\n",
        "Date_added : Date it was added on Netflix\n",
        "\n",
        "Release_year : Actual Releaseyear of the movie / show\n",
        "\n",
        "Rating : TV Rating of the movie / show\n",
        "\n",
        "Duration : Total Duration - in minutes or number of seasons\n",
        "\n",
        "Listed_in : Genere\n",
        "\n",
        "Description: The Summary description"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "!pip install xplotter --upgrade\n",
        "!pip install pycountry-convert\n",
        "!pip install geopy\n",
        "!pip install folium\n",
        "!pip install geopandas\n",
        "###################\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import missingno as msno\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Importing Date & Time util modules\n",
        "from dateutil.parser import parse\n",
        "\n",
        "import collections as c\n",
        "\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#for nlp\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import string\n",
        "string.punctuation\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "\n",
        "# Importing Date & Time util modules\n",
        "from dateutil.parser import parse\n",
        "\n",
        "import collections as c\n",
        "\n",
        "\n",
        "# Xplotter\n",
        "from xplotter.insights import *\n",
        "from xplotter.formatter import format_spines\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/unsupervised netflix project/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "30k1MZSHpKGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for null values and missing values information\n",
        "def null_info(df1):\n",
        "  mv=df.isnull().sum()\n",
        "  mv_percentage=mv*100/len(df)\n",
        "  unique_value=df.nunique()\n",
        "  ni_table=pd.concat([mv,mv_percentage,unique_value],axis=1)\n",
        "  ni_table=ni_table.rename(columns={0:'no. of null values',1:'percentage of null value',2:'no. of unique values'})\n",
        "  ni_table['data type']=df.dtypes\n",
        "  ni_table=ni_table.sort_values('percentage of null value',ascending=False).round(1)\n",
        "  print('data frame has '+ str(df1.shape[1])+\" columns and \"+ str(df.shape[0]),'rows')\n",
        "  return ni_table"
      ],
      "metadata": {
        "id": "NfY9U77XrLqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_info(df)"
      ],
      "metadata": {
        "id": "1ZNgv85ptx3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import missingno as msno        #this library is used to visualised the the number of missing values\n",
        "msno.bar(df)"
      ],
      "metadata": {
        "id": "XziQ1EZIvNGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-From the above we find that director,cast,contry,date_added and rating contain the null/missing values in it .And director contain highest no. of misssing values in it.\n",
        "\n",
        "-There is no duplicated values present in the given data set.\n",
        "\n",
        "-All the data have data types of 'objects' except 'release yeear' which is integers type."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frome the above we find that\n",
        " (Show_id-\tUnique id for every movie/TV show,\n",
        " \n",
        " Release_year-\tActual release year of the show,\n",
        "\n",
        "Type\tIdentifier - Movie/tv show,\n",
        "\n",
        "Rating-\tTV rating of the show,\n",
        "\n",
        "Title-\tTitle of the show/movie\t,\t\n",
        "\n",
        "Duration\t-Total duration in minutes or number of seasons,\n",
        "\n",
        "Director-\tDirector of the show,\t\t\n",
        "\n",
        "Listed_in-\tGenre,\n",
        "\n",
        "Cast-\tActors involved\t,\n",
        "\n",
        "Description\t-Summary description,\n",
        "\n",
        "Country\t-Country of production,\t\t\n",
        "\n",
        "Date_added-\tDate it was added on netflix.)\n",
        "\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "null_info(df)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#creting new feature which store  date,month,and year separetly.\n",
        "#first we have to convert the 'date_added' in datetime format.\n",
        "df['date_added']=pd.to_datetime(df['date_added'])\n",
        "#day\n",
        "df['day']=df['date_added'].dt.day\n",
        "#month\n",
        "df['month']=df['date_added'].dt.month\n",
        "#year\n",
        "df['year']=df['date_added'].dt.year\n",
        "\n",
        "\n",
        "#  Creating two extra columns \n",
        "tv_shows=df[df['type']=='TV Show']\n",
        "movies=df[df['type']=='Movie']\n",
        "\n",
        "# duration manipulation for movie\n",
        "movies.duration = movies.duration.str.replace(' min','').astype(int)\n",
        "\n",
        "# duration manipulation for tv shows\n",
        "tv_shows.rename(columns={'duration':'seasons'}, inplace=True)\n",
        "tv_shows.replace({'seasons':{'1 Season':'1 Seasons'}}, inplace=True)\n",
        "tv_shows.seasons = tv_shows.seasons.str.replace(' Seasons','').astype(int)\n",
        "\n",
        "# Splitting datasets for movies and tv show\n",
        "df_movies = df[df['type']=='Movie'].copy()\n",
        "df_shows = df[df['type']=='TV Show'].copy()\n",
        "#fixing data type\n",
        "# Changing the values in the rating column\n",
        "rating_map = {'TV-MA':'Adults',\n",
        "              'R':'Adults',\n",
        "              'PG-13':'Teens',\n",
        "              'TV-14':'Young Adults',\n",
        "              'TV-PG':'Older Kids',\n",
        "              'NR':'Adults',\n",
        "              'TV-G':'Kids',\n",
        "              'TV-Y':'Kids',\n",
        "              'TV-Y7':'Older Kids',\n",
        "              'PG':'Older Kids',\n",
        "              'G':'Kids',\n",
        "              'NC-17':'Adults',\n",
        "              'TV-Y7-FV':'Older Kids',\n",
        "              'UR':'Adults'}\n",
        "\n",
        "df['rating'].replace(rating_map, inplace = True)\n",
        "df['rating'].unique()\n",
        "\n",
        "df['type'] = pd.Categorical(df['type'])\n",
        "\n",
        "# changing dtype to string\n",
        "df['type']= df['type'].astype('string')\n",
        "df['title']= df['title'].astype('string')\n",
        "df['director']=df['director'].astype('string')\n",
        "df['cast']=df['cast'].astype('string')\n",
        "df['country']= df['country'].astype('string')\n",
        "df['rating']= df['rating'].astype('string')\n",
        "df['listed_in']= df['listed_in'].astype('string')\n",
        "df['description']= df['description'].astype('string')\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "wSOk7MnqyKQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-First we have to convert the date_added data type to datetime format.\n",
        "\n",
        "-Split the date_added to day,month,year and create a separate columns.\n",
        "\n",
        "-Here we separate the data into two categories on the basis of type. \n",
        "\n",
        "-In duration for movie min are get removed and used only integers.\n",
        "\n",
        "-Similrly for tv shows seasons are removed and only integers vales are used.\n",
        "\n",
        "-On the basis of rating, divide it into four categories kids,older kids,teens and adults.\n",
        "\n",
        "-Converted all the data types in  string except date which is in datetime format.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Univariate analysis:**"
      ],
      "metadata": {
        "id": "y4qskcI_ulgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Balance of Netflix products\n",
        "plot_donut_chart(df=df, col='type', colors=['blue','green'], title='Does Netflix had more \\nMovies or TV Shows in 2019?')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donut charts are the modified version of Pie Charts with the area of center cut out. A donut is more concerned about the use of area of arcs to represent the information in the most effective manner instead of Pie chart which is more focused on comparing the proportion area between the slices."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are ~70% movies and ~30% are tv shows."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more no. of movies with respect to tv shows ,if we see in term of business aspect we have to work on tv shows to make it more atttractive so that demand of tv shows are increses which leads to incresse the no. of tv shows."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Graph for top 10 actors\n",
        "actors = df[df.cast != 'No Cast'].set_index('title').cast.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "sns.countplot(y = actors, order=actors.value_counts().index[:10], palette='Paired')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are explainable because of their linear data representation. we have used bar charts to represent dependent variable ditribution, Vertical bar charts are useful to compare different categorical or discrete variables as long as they are too many categories to compare."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above graph shows top 10 actors who have worked in loads of movies and TV shows."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will help to cast a better actor  who have better experience,fan base and popularity."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Graph for top 10 directors\n",
        "directors = df[df.director != 'No Director'].set_index('title').director.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "sns.countplot(y = directors, order=directors.value_counts().index[:10], palette='Set2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are explainable because of their linear data representation. we have used bar charts to represent dependent variable ditribution, Vertical bar charts are useful to compare different categorical or discrete variables as long as they are too many categories to compare."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This visual shows Jan suter and Raul Campos are the leading directors, with abundant experience."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will help to choose a better and experience director for the movie and tv series."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization \n",
        "# #Analysing top10 genre of the movies/tv shows\n",
        "\n",
        "sns.countplot(y=df['listed_in'],data=df,order=df['listed_in'].value_counts().index[0:10],  palette='Paired',)\n",
        "plt.title('Top10 Genre of Movies/tv shows')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are explainable because of their linear data representation. we have used bar charts to represent dependent variable ditribution, Vertical bar charts are useful to compare different categorical or discrete variables as long as they are too many categories to compare."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart found that documentaries  fallowed by comedy and fallowed drama are the most popular genere of the tv shows as well as movies."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will helps to choose best genere to make movies as well as tv shows ."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Visualizing the year in which the movie / tv show was released\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(18,6))\n",
        "sns.set(style='whitegrid')\n",
        "fig.set_facecolor(\"lightblue\")\n",
        "sns.countplot(data =df,x = 'year',palette =\"Set2\")\n",
        "plt.title('Movies added to Netflix each year ', fontsize=10)\n",
        "plt.ylabel('No. of movies added to Netflix', fontsize=10)\n",
        "plt.xlabel('Year -> (Movies added to Netflix) ', fontsize=10)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is often used to illustrate the major features of the distribution of the data in a convenient form. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above find that highest no. of movies and tv shows are made in the years of 2019 and decreases in 2020 - 2021 due to covid."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will to help to conclude that no. of making of movies and tv shows increases very fast from 2014 to 2019 after that due to the impact of covid it deceases very drastically."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Top 10 countries with the highest number movies / TV shows in the dataset\n",
        "plt.figure(figsize=(10,5))\n",
        "df[~(df['country']=='Unknown')].country.value_counts().nlargest(10).plot(kind='barh')\n",
        "\n",
        "plt.title(' Top 10 countries with the highest number of shows')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar graph summarises the large set of data in simple visual form.\n",
        "It displays each category of data in the frequency distribution.\n",
        "It clarifies the trend of data better than the table.\n",
        "It helps in estimating the key values at a glance."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart find that united states produces more no. of movies and tv shows fallowed by india and united kindom."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will help to choose the best country for producing and making movies ."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Number of shows on Netflix for different age groups\n",
        "plt.figure(figsize=(10,5))\n",
        "a=df.rating.value_counts().plot(kind='barh',color='#7eb54e')\n",
        "plt.title('Number of shows on Netflix for different age groups')"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sReW8zxh8NPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar graph summarises the large set of data in simple visual form. It displays each category of data in the frequency distribution. It clarifies the trend of data better than the table. It helps in estimating the key values at a glance."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the given plot found the most of the shows and movies are moade for adults audiences and fallowed by the young audiences ,older kids and kids."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will to creator to choose audience and made the movies and tv shows acoording to the interest of the audiences."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "plt.plot(figsize=(20, 5))\n",
        "g1 = sns.distplot(movies.duration, color='blue');\n",
        "g1.set_xticks(np.arange(0,360,30))\n",
        "g1.set_title(\"Duration Distribution for Netflix Movies\")\n",
        "g1.set_ylabel(\"% of All Netflix Movies\")\n",
        "g1.set_xlabel(\"Duration (minutes)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(figsize=(50, 100))\n",
        "g2 = sns.distplot(tv_shows.seasons, color='blue');\n",
        "g2.set_xticks(np.arange(0,16))\n",
        "g2.set_title(\"Numbers of seasons of a tv shows\")\n",
        "g2.set_ylabel(\"% of All tv shows\")\n",
        "g2.set_xlabel(\"seasons\")"
      ],
      "metadata": {
        "id": "DLr1aKYSDu2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histograms have been used to represent distribution of columns having numerical data, histogram is basically used to represent data provided in a form of some groups.It is accurate method for the graphical representation of numerical data distribution.It is a type of bar plot where X-axis represents the bin ranges while Y-axis gives information about frequency."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the movies have duration of 90 mins.\n",
        "Most of the TV shows have one season, but this number goes on decreasing with increase in number seasons."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will help to decide the length of the movies as well as no. of seasons for a tv shows."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bivariate Analysis:**"
      ],
      "metadata": {
        "id": "SUf_fYOKLvkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "#Analysing top15 countries with most content of mvies and tv shows separetely \n",
        "plt.figure(figsize=(18,5))\n",
        "sns.countplot(x=df['country'],order=df['country'].value_counts().index[0:15],hue=df['type'], palette='Set2')\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 15 countries with most contents', fontsize=15, fontweight='bold')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple bar charts are extremely useful if you want to present a large amount of information in a small amount of space. Instead of having only one bar at each division on the horizontal axis."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here: United States has highest number of contents in Netflix followed by India."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that very large variety of content is not available for India and severl other countries thus missing variety should be made available for other countries to provide better options for audience and to increase engagement."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Building a wordcloud for the movie descriptions\n",
        "words = ''\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# iterate through the csv file\n",
        "for value in df.description.values:\n",
        "    \n",
        "    # typecaste each val to string\n",
        "    value = str(value)\n",
        "\n",
        "    # split the value\n",
        "    tokens = value.split()\n",
        "    \n",
        "    # Converts each token into lowercase\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "    \n",
        "    words += \" \".join(tokens)+\" \"\n",
        "\n",
        "wordcloud = WordCloud(width = 700, height = 700,\n",
        "                background_color ='white',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10).generate(words)\n",
        "\n",
        "\n",
        "# plot the WordCloud image                      \n",
        "plt.figure(figsize = (15,6), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A word cloud (also known as a tag cloud or text cloud) is a visual representation of a text, in which the words appear bigger the more often they are mentioned. Word clouds are great for visualizing unstructured text data and getting insights on trends and patterns."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Family, Life, Find, Love are the most used words in the description column."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# visualizing the movies and tv_shows based on the release year\n",
        "movies_year =movies['year'].value_counts().sort_index(ascending=False)\n",
        "tvshows_year =tv_shows['year'].value_counts().sort_index(ascending=False)\n",
        "sns.set(font_scale=1.4)\n",
        "movies_year.plot(figsize=(12, 8), linewidth=2.5, color='blue',label=\"Movies per year\",ms=3)\n",
        "tvshows_year.plot(figsize=(12, 8), linewidth=2.5, color='green',label=\"TV Shows per year\")\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.title(\"Production growth yearly\", y=1.02, fontsize=22);"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps to show small shifts that may be getting hard to spot in other graphs.\n",
        "It helps show trends for different periods.\n",
        "They are easy to understand.\n",
        "To compare data, more than one line can be plotted on the same axis."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the year 2019 more number of movies were released as compared to TV shows."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are less number of TV shows available, therefore we should promote TV shows production to ensure better audience retention due to more duration and story complexity."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multivariate Analysis:**"
      ],
      "metadata": {
        "id": "61dzbpiJMg-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 coorelation heatmap visualization code\n",
        "df['count'] = 1\n",
        "data = df.groupby('country')[['country','count']].sum().sort_values(by='count',ascending=False).reset_index()[:10]\n",
        "data = data['country']\n",
        "heatmap_df = df.loc[df['country'].isin(data)]\n",
        "heatmap_df = pd.crosstab(heatmap_df['country'],heatmap_df['rating'],normalize = \"index\").T\n",
        "\n",
        "# Plotting the heatmap\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "country_order2 = ['United States', 'India', 'United Kingdom', 'Canada', 'Japan', 'France', 'South Korea', 'Spain',\n",
        "       'Mexico']\n",
        "age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "sns.heatmap(heatmap_df.loc[age_order,country_order2],cmap='icefire',annot=True, linecolor='Black', linewidths=1.0,square=True, cbar=False,\n",
        "            fmt='1.0%',vmax=.6,vmin=0.05,ax=ax,annot_kws={\"fontsize\":12})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlations are best represented using heat maps. Heat Maps is used to better visualize the volume of locations/events within a dataset and assist in directing viewers towards areas on data visualizations that matter most."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum adult content is from Spain.\n",
        "\n",
        "Maximum teen content is from India.\n",
        "\n",
        "Maximum older kids content is from Japan.\n",
        "\n",
        "Maximum kids content is from Canada."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes it will to create a specific content for a particular audience in a particular country."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Checking relation among all the columns once to do further analysis \n",
        "sns.pairplot(data=df)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie release is not biased with respect to days or months, and no other relationship or trends can be observed from the pairplots."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis(H0) - The Country is independent of Target_ages.\n",
        "\n",
        "Alternative Hypothesis(H1) - The Country is associated with Target_ages."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making copy of df_clean_frame\n",
        "hypothesis=df.copy()\n",
        "hypothesis = hypothesis[hypothesis[\"type\"] == \"Movie\"]\n",
        "ratings_ages = {\n",
        "    'TV-PG': 'Older Kids',\n",
        "    'TV-MA': 'Adults',\n",
        "    'TV-Y7-FV': 'Older Kids',\n",
        "    'TV-Y7': 'Older Kids',\n",
        "    'TV-14': 'Teens',\n",
        "    'R': 'Adults',\n",
        "    'TV-Y': 'Kids',\n",
        "    'NR': 'Adults',\n",
        "    'PG-13': 'Teens',\n",
        "    'TV-G': 'Kids',\n",
        "    'PG': 'Older Kids',\n",
        "    'G': 'Kids',\n",
        "    'UR': 'Adults',\n",
        "    'NC-17': 'Adults'\n",
        "}\n",
        "hypothesis['target_ages'] = hypothesis['rating'].replace(ratings_ages)\n",
        "# Let's see unique target ages \n",
        "hypothesis['target_ages'].unique()\n",
        "hypothesis['target_ages'] = pd.Categorical(hypothesis['target_ages'], categories=['Kids', 'Older Kids', 'Teens', 'Adults'])\n",
        "# From duration feature extractin string part and after extracting Changing the object type to numeric\n",
        "hypothesis['duration']= hypothesis['duration'].str.extract('(\\d+)')\n",
        "hypothesis['duration'] = pd.to_numeric(hypothesis['duration'])\n",
        "# Head of df_\n",
        "hypothesis.head(3)\n",
        "# Group_by duration and target_ages                 \n",
        "group_by_= hypothesis[['duration','target_ages']].groupby(by='target_ages')\n",
        "# Mean of group_by variable\n",
        "group=group_by_.mean().reset_index()\n",
        "group\n",
        "# In A and B variable grouping values \n",
        "one = group_by_.get_group('Kids')\n",
        "two = group_by_.get_group('Older Kids')\n",
        "# Mean and std. calutation for kids and older kids variables\n",
        "Mean1 = one.mean()\n",
        "SD1 = one.std()\n",
        "\n",
        "Mean2= two.mean()\n",
        "SD2 = two.std()\n",
        "\n",
        "print('Mean for movies rated for Kids {} \\n Mean for  movies rated for older kids {}'.format(Mean1,Mean2))\n",
        "print('Std deviation for  movies rated for Older Kids {} \\n Std for  movies rated for kids {}'.format(SD2,SD1))"
      ],
      "metadata": {
        "id": "WWuGehP89qnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "cont_table  = pd.crosstab(hypothesis.country, hypothesis.target_ages)\n",
        "cont_table"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "X2, p_value, degree_freedom, expected_values = chi2_contingency(cont_table, correction = True)\n",
        "print(\"X2: {0}, P-Value: {1}\".format(X2,p_value)) # IF P VALUE IS LESS THAN ALPHA VALUE(X2) THAN THERE IS ENOUGHT EVIDENCE TO REJECT NULL HYPOTHESIS"
      ],
      "metadata": {
        "id": "B5j-cJiV1ya4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus we reject null hypothesis, Country is associated with target ages."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chi-square test"
      ],
      "metadata": {
        "id": "4bsCsgzn2Ap-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A chi-square test is a statistical test used to compare observed results with expected results. The purpose of this test is to determine if a difference between observed data and expected data is due to chance, or if it is due to a relationship between the variables you are studying."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis(H0) - The Duration is independent of Target_ages.\n",
        "\n",
        "Alternative Hypothesis(H1) - The Duration is associated with Target_ages."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "cont_table_1  = pd.crosstab(hypothesis.target_ages, hypothesis.duration)\n",
        "cont_table_1"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "X2, p_value, degree_freedom, expected_values = chi2_contingency(cont_table_1, correction = True)\n",
        "print(\"X2: {0}, P-Value: {1}\".format(X2,p_value))  # IF P VALUE IS LESS THAN ALPHA VALUE(X2) THAN THERE IS ENOUGHT EVIDENCE TO REJECT NULL HYPOTHESIS"
      ],
      "metadata": {
        "id": "tjOURHCM-Mz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We reject the null hyposthesis, Duration is assosiated with target ages."
      ],
      "metadata": {
        "id": "MkJ92cbW-Tq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi Square Test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A chi-square test is a statistical test used to compare observed results with expected results. The purpose of this test is to determine if a difference between observed data and expected data is due to chance, or if it is due to a relationship between the variables you are studying."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 Hypothesis : Movies for kids and older kids are not two hours long.\n",
        "\n",
        "H1 Hypothesis : Movies for kids and older kids are two hours long."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Import stats \n",
        "from scipy import stats\n",
        "# Length of groups and DOF\n",
        "len1 = len(one)\n",
        "len2= len(two)\n",
        "print(len1,len2)\n",
        "\n",
        "dof = len1+len2-2\n",
        "print('dof',dof)\n",
        "\n",
        "sp_2 = ((len2-1)*SD1**2  + (len1-1)*SD2**2) / dof\n",
        "print('SP_2 =',sp_2)\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "print('SP',sp)\n",
        "\n",
        "# t-value\n",
        "t_value = (Mean1-Mean2)/(sp * np.sqrt(1/len1 + 1/len2))\n",
        "print('t-value',t_value[0])"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.t.ppf(0.025,dof)"
      ],
      "metadata": {
        "id": "U31lDCSG-1kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.t.ppf(0.975,dof)   #If the absolute value of the t-value is greater than the critical value, THEN WE REJECT THE NULL HYPOTHESIS."
      ],
      "metadata": {
        "id": "LYT1a6MY-37o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus we reject NUll hypothesis, movies for kids and older kids are two hours long."
      ],
      "metadata": {
        "id": "iJEW4aTc-8kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T-Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A t test is a statistical test that is used to compare the means of two groups. It is often used in hypothesis testing to determine whether a process or treatment actually has an effect on the population of interest, or whether two groups are different from one another."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "null_info(df)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['date_added','day','year','month','rating'],inplace=True)\n",
        "df['director'].fillna('unknown',inplace=True)\n",
        "df['cast'].fillna('unknown',inplace=True)\n",
        "df['country'].fillna(value=df['country'].mode()[0],inplace=True)"
      ],
      "metadata": {
        "id": "R-OxxQECFaq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_info(df)"
      ],
      "metadata": {
        "id": "2Tt8G7oyIf4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  'director','cast' null values are replaced by 'unknown' where as 'country' null values are replaced by most repeated country. And 'date_added','day','year','month','rating' null values are drop beacause the no. of null values are very small in comparion to data ."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Expand ContractioN\n",
        "df1=df[['title']]\n",
        "df1['cluster_data'] =(df['description'] + ' ' +df['listed_in'] + ' ' + df['director'] + ' ' + df['cast'] + ' ' + df['country'] + ' ' + df['rating']).astype(str)\n",
        "df1.set_index('title', inplace = True)\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df1['cluster_data'] = df1['cluster_data'].str.lower()\n",
        "df1['cluster_data']"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(text):\n",
        "  'a function is removing punctuation'\n",
        "  import string\n",
        "  translator=str.maketrans('','',string.punctuation)\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['cluster_data']=df1['cluster_data'].apply(remove_punctuation)\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "EgDh6J5GAB4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopword')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('english')"
      ],
      "metadata": {
        "id": "HsaFP-BBA9QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords(text):\n",
        "  'a function is removing stop words'\n",
        "  text=[word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "  return \" \".join(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "t81WEiASBmC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['cluster_data']=df1['cluster_data'].apply(stopwords)\n",
        "df1.cluster_data.head()"
      ],
      "metadata": {
        "id": "KzzBbhjNC3yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "#creating an object for stemming function\n",
        "stemmer=SnowballStemmer('english')\n",
        "def stemming(text):\n",
        "  'a fuction which steam each word in the given text'\n",
        "  text=[stemmer.stem(word) for word in text.split()]\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['cluster_data']=df1['cluster_data'].apply(stemming)\n",
        "df1.cluster_data.head()"
      ],
      "metadata": {
        "id": "lSOR3d54IS03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = df1.cluster_data"
      ],
      "metadata": {
        "id": "OSuLq_9LKMk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Length(listed_in)'] = df['listed_in'].apply(lambda x: len(x))\n",
        "df['Length(description)'] = df['description'].apply(lambda x: len(x))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QyCStpHeM14s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used stemming for normalization of text. As Stemming removes last few characters from a word, often leading to incorrect meanings and spelling."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "df['description']=df['description'].fillna('')\n",
        "tfidf=TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#tfidf matrix\n",
        "tfidf_matrix=tfidf.fit_transform(df['description'])\n",
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF will transform the text into meaningful representation of integers or numbers which is used to fit machine learning algorithm for predictions."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "x_features=df[['Length(description)','Length(listed_in)']]\n",
        "stdscaler=preprocessing.StandardScaler()\n",
        "x_features.describe()"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling of data\n",
        "\n",
        "X_rescale=stdscaler.fit_transform(x_features)\n",
        "X=X_rescale\n",
        "silhouette_score_ = [  ]\n",
        "range_n_clusters = [i for i in range(2,16)]"
      ],
      "metadata": {
        "id": "EQnVhbHYS-sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    silhouette_score_.append([int(n_clusters) , round(score , 2)])\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "lFEljZ6YXITS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimension reduction is very important as\n",
        "\n",
        "Fewer features mean less complexity.\n",
        "\n",
        "You will need less storage space because you have fewer data.\n",
        "\n",
        "Fewer features require less computation time.\n",
        "\n",
        "Model accuracy improves due to less misleading data.\n",
        "\n",
        "Algorithms train faster thanks to fewer data.\n",
        "\n",
        "Reducing the data setâ€™s feature dimensions helps visualize the data faster.\n",
        "\n",
        "It removes noise and redundant features."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "pca = PCA(random_state=42)\n",
        "pca.fit(X)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cumulative explained variance vs number of components\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title('PCA - Cumulative explained variance vs number of components')\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')"
      ],
      "metadata": {
        "id": "ZQCOf0WZX8Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reducing the dimensions using pca\n",
        "pca = PCA(n_components=2,random_state=42)\n",
        "pca.fit(X)\n",
        "x_pca = pca.transform(X)"
      ],
      "metadata": {
        "id": "olj43vcAYISc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of transformed vectors\n",
        "x_pca.shape"
      ],
      "metadata": {
        "id": "-IPVwWV_YNl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Principle Component Analysis as the key aim of PCA is to reduce the number of variables of a data set, while preserving as much information as possible."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 ( K means)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 K means\n",
        "k_clusters = 4\n",
        "# Fit the Algorithm\n",
        "model = KMeans(n_clusters=k_clusters, init='k-means++', n_init=10, max_iter=600, tol=0.000001, random_state=42)\n",
        "model.fit(X)\n",
        "# Predict on the model\n",
        "clusters = model.predict(X)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the silhouette score :\n",
        "score = silhouette_score(X, clusters)\n",
        "print(\"Silhouette score is {}\".format(score))"
      ],
      "metadata": {
        "id": "w4xq98nNcbCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a seperate column for the clusters L:\n",
        "df[\"Cluster Label\"] = clusters\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zDuK4yscci5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "temp = pd.DataFrame(silhouette_score_ , columns = [\"n clusters\" , \"silhouette score\"])\n",
        "temp = temp.sort_values( \"silhouette score\" , ascending = False )\n",
        "temp.head(15) "
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_n_clusters = [i for i in range(2,16)]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns :\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "    \n",
        "\n",
        "    # Compute the silhouette scores for each sample :\n",
        "\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them :\n",
        "\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.hsv_r(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle :\n",
        "\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "    #ax1.set_facecolor(\"white\")\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "    ax1.grid(False)\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values :\n",
        "\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed :\n",
        "\n",
        "    colors = cm.hsv_r(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters :\n",
        "\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    # Draw white circles at cluster centers :\n",
        "\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "  \n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    ax2.grid(False)\n",
        "    #ax2.set_facecolor(\"white\") :\n",
        "    \n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qOPviLf_dM4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 ( Elbow Curve)"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 2 Implementation\n",
        "sum_of_square_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    sum_of_square_dist[k] = km.inertia_\n",
        "    \n",
        "#Plot the graph for the sum of square distance values and Number of Clusters :\n",
        "\n",
        "sns.pointplot(x = list(sum_of_square_dist.keys()), y = list(sum_of_square_dist.values()),color=\"red\")\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "kmeans = KMeans(n_clusters = 6 )\n",
        "# fit the model\n",
        "kmeans.fit(X)\n",
        "# predict on the model\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "hIl9xXLOgIGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.title('description and listed_in')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=60, cmap='rainbow')\n",
        "plt.grid(color='r')\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='blue', s=200, alpha=0.5)\n"
      ],
      "metadata": {
        "id": "Dhs3ljfIgOii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation : K-means clustering shows that '4' will be optimum no of clusters with the silhoutte score of 0.45. But we selected Optimum cluster number as 6 after Elbow curve cross validation."
      ],
      "metadata": {
        "id": "Ww1R9E0Uhcbx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3( DBSCAN)"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=20).fit_predict(X)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.grid(color='r')\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred, cmap='rainbow')"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: DBSCAN algorithm identifies the dense region by grouping together data points that are close to eachother based on distance measurement. we observed that cluster is getting dense towards the centre, and is bit spreaded towards the extreme ends. Thus, it will not be suitable algorithm for clustering studies."
      ],
      "metadata": {
        "id": "Ld12Gt_oi23O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWgjPnYQi2ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 ( Hierarchical clustering)"
      ],
      "metadata": {
        "id": "8349Tk17j2EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  importing sch :\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.style.use('default')\n",
        "sch.set_link_color_palette(['#b30000','#996600', '#b30086'])\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'),above_threshold_color=\"green\")\n",
        "plt.axhline(y=70, color='r', linestyle='--')\n",
        "# find largest vertical distance we can make without crossing any other horizontal line :\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8J8Ew5Tsj-6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Here we can observe 3-4 major clusters and their smilarities."
      ],
      "metadata": {
        "id": "YCz1IomIkI_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 ( Agglomerative Clustering)"
      ],
      "metadata": {
        "id": "mxF1UExUkX5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 5 implementation\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 6, affinity = 'euclidean', linkage = 'ward')\n",
        "# predict on the model\n",
        "y_hc = hc.fit_predict(X)"
      ],
      "metadata": {
        "id": "HKXM_TZFkewQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the clusters :\n",
        "plt.figure(figsize=(14,10))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = '1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = '2')\n",
        "plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = '3')\n",
        "plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'yellow', label = '4')\n",
        "plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'black', label = '5')\n",
        "plt.scatter(X[y_hc == 5, 0], X[y_hc == 5, 1], s = 100, c = 'pink', label = '6')\n",
        "plt.grid(color='r')\n",
        "plt.title('Clusters of content')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xFqluEEvkjIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recommendation System**"
      ],
      "metadata": {
        "id": "mxDL569IkZjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pd.DataFrame(tfidf_matrix[0].T.todense(), columns=[\"tf-idf score\"])\n"
      ],
      "metadata": {
        "id": "wc_c6vlfkdVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CountVectorizer: Convert a collection of text documents to a matrix of token counts\n",
        "#TfidfTransformer: Transform a count matrix to a normalized tf or tf-idf representation\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([('cv', CountVectorizer(stop_words='english')),\n",
        "                ('tfid', TfidfTransformer())]).fit(df['description'])\n",
        "\n",
        "print(\"CountVectorizer_matrix: {}\".format(pipe['cv'].transform(df['description']).toarray()))\n",
        "print(\"CountVectorizer_matrix: {}\".format(pipe['tfid'].idf_))"
      ],
      "metadata": {
        "id": "MAKOIsjakjaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The lower the IDF value of a word, the less unique it is to any particular document\n",
        "pd.DataFrame(pipe['tfid'].idf_, columns=['idf_weights'])"
      ],
      "metadata": {
        "id": "LdZQqQsOkl_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine similarity recommender**"
      ],
      "metadata": {
        "id": "U1azEEbdkqrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)"
      ],
      "metadata": {
        "id": "5ZZVWbebkqUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "programme_list=df['title'].to_list()"
      ],
      "metadata": {
        "id": "-fLHxYEAkycl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(title, cosine_similarity=cosine_sim):\n",
        "    index = programme_list.index(title)\n",
        "    sim_score = list(enumerate(cosine_sim[index]))\n",
        "    sim_score = sorted(sim_score, key= lambda x: x[1], reverse=True)[1:11] #position 0 is the movie itself, thus exclude\n",
        "    recommend_index = [i[0] for i in sim_score]\n",
        "    rec_movie = df['title'].iloc[recommend_index]\n",
        "    rec_score = [round(i[1],4) for i in sim_score]\n",
        "    rec_table = pd.DataFrame(list(zip(rec_movie,rec_score)), columns=['Recommend programme','Similarity(0-1)'])\n",
        "    return rec_table"
      ],
      "metadata": {
        "id": "5lkFjEsFk1Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend(\"3 Idiots\")"
      ],
      "metadata": {
        "id": "uX8qLcGHk3lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that algorithm is accuratly recommending College Romance, Engineering girls which was basically theme of 3 idiots. Thus, the cluster is good."
      ],
      "metadata": {
        "id": "06c-NBA3k65r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommend(\"Indiana Jones and the Last Crusade\")"
      ],
      "metadata": {
        "id": "a2e-zMg6k-1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, recommendation is also giving smiliar title movies if sequels and prequels are available."
      ],
      "metadata": {
        "id": "h-6GlNJzlFGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix have ~70% of movies and 30% of TV_shows in 2019.\n",
        "\n",
        "Comedy is most popular genre in Netflix, across all content.\n",
        "\n",
        "Netflix focuses to add new content majorly towards end of current year and start of new year.\n",
        "\n",
        "A sudden drop was obserbed after 2020 in count of new content, Which is because of covid pandemic.\n",
        "\n",
        "India is on 2nd place as compared to content availability, maximum content is available for United states.\n",
        "\n",
        "There are almost ~30% of netflix original movies and ~50 % TV-shows.\n",
        "Content category and countries:\n",
        "\n",
        "Maximum adult content is from Spain.\n",
        "\n",
        "Maximum teen content is from India.\n",
        "\n",
        "Maximum older kids content is from Japan.\n",
        "\n",
        "Maximum kids content is from Canada.\n",
        "\n",
        "All of this insights will be neccesary for business development and SWOT analysis.\n",
        "\n",
        "Conclusion for clustering.\n",
        "\n",
        "We tried 5 models for ML i.e.\n",
        "\n",
        "K-means clustering\n",
        "\n",
        "Elbow curve\n",
        "\n",
        "DBSCAN\n",
        "\n",
        "Hierarchical clustering\n",
        "\n",
        "Agglomerative clustering\n",
        "\n",
        "K-means clustering shows that '4' will be optimum no of clusters with the silhoutte score of 0.45. But we selected Optimum cluster number as 6 after Elbow curve cross validation.\n",
        "\n",
        "Thus K-means clustering will be best for this data set.\n",
        "\n",
        "Cosine based recommender system was working really well."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}